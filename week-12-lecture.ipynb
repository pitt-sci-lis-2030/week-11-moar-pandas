{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 12 - Moar Pandas\n",
    "\n",
    "\n",
    "- Subsetting data\n",
    "    - Masking by values\n",
    "- Merging Data\n",
    "    - Merging and Joining\n",
    "    - Joining with Real Data\n",
    "- Grouping data\n",
    "    - Split Apply Combine\n",
    "    - Split Apply Combine with Real Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas so we can do stuff\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subsetting Data\n",
    "\n",
    "* It is sometimes helpful to think of a Pandas Dataframe as a little database. \n",
    "* There is data and information stored in the Pandas Dataframe (or Series) and you want to *retrieve* it.\n",
    "* Pandas has multiple mechanisms for getting specific bits of data and information from its data structures. \n",
    "\n",
    "### Masking: Filtering by Values\n",
    "\n",
    "* The most common is to use *masking* to select just the rows you want. \n",
    "* Masking is a two stage process, first you create a sequence of boolean values (corresponding to rows in your data) based upon a conditional expression--which you can think of as a \"query\"--and then you index your dataframe using that boolean sequence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data into a pandas dataframe\n",
    "order_data  = pd.read_csv(\"files/chipotle.tsv\", sep=\"\\t\")\n",
    "# inspect the dataframe\n",
    "order_data.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the chipotle order data\n",
    "order_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at all the columns\n",
    "order_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* How might we only look at particular orders?\n",
    "* First step is to create a *query mask*, a list of `True/False` values for rows that satisfy a particular condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a query mask for chicken bowls\n",
    "query_mask = order_data['item_name'] == \"Chicken Bowl\"\n",
    "\n",
    "#look at the first 20 items to see what matches\n",
    "query_mask.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This tells us the row id and True or False if the item type equals chicken bowl\n",
    "* We can look up that row by index and see if it is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_data.iloc[19]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Yup! So now that we know the mask works, we can create a *subset* of our data containing chicken bowls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chicken_bowls = order_data[query_mask]\n",
    "chicken_bowls.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now you can do things like calculate the average price for chicken bowl orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean price for chicken bowls\n",
    "chicken_bowls['item_price'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See how many chicken bowls people order\n",
    "chicken_bowls['quantity'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can also combine query masks using boolean logic\n",
    "* Can we look at just the chicken bowl orders that were less than $10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a query mask for chicken bowls\n",
    "item_query_mask = order_data['item_name'] == \"Chicken Bowl\"\n",
    "# create a query mask for cheap orders\n",
    "price_query_mask = order_data['item_price'] < 10\n",
    "\n",
    "# apply both query masks using boolean AND\n",
    "cheap_chicken_bowls = order_data[item_query_mask & price_query_mask]\n",
    "cheap_chicken_bowls.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Median price for cheap chicken bowls\n",
    "cheap_chicken_bowls['item_price'].median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Query masks can be used to filter and create subsets of data\n",
    "* Note, this method of subsetting data creates what is called a \"view\" of the data\n",
    "* You are basically working with a big slice of the original dataframe, not a separate copy of the data\n",
    "* This means if you try an do transformations on that view, you will get an error\n",
    "* For more information, [see the pandas documentation](http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cheap_chicken_bowls['half_price'] = cheap_chicken_bowls['item_price'] / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_of_cheap_chicken_bowls = cheap_chicken_bowls.copy()\n",
    "copy_of_cheap_chicken_bowls['half_price'] = copy_of_cheap_chicken_bowls['item_price'] / 2\n",
    "copy_of_cheap_chicken_bowls.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging Data\n",
    "\n",
    "* Bringing disparate datasets together is one of the more powerful features of Pandas\n",
    "* Like with Python lists, you can `append()` and `concat()` Pandas `Series` and `Dataframes`\n",
    "* The `concat` is a module function, you call it directly from the pandas module (usually called `pd`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatinate two series together\n",
    "ser1 = pd.Series(['A', 'B', 'C'], index=[1, 2, 3])\n",
    "ser2 = pd.Series(['D', 'E', 'F'], index=[4, 5, 6])\n",
    "pd.concat([ser1, ser2]) #note the Seres are passed as a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# order matters\n",
    "pd.concat([ser2, ser1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatinate two dataframes\n",
    "df1 = pd.DataFrame({\"A\":[\"A1\", \"A2\"],\n",
    "                    \"B\":[\"B1\",\"B2\"]},index=[1,2])\n",
    "df2 = pd.DataFrame({\"A\":[\"A3\", \"A4\"],\n",
    "                    \"B\":[\"B3\",\"B4\"]},index=[3,4])\n",
    "pd.concat([df1,df2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Pandas will automatically line up matching indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatinate dataframes horizontally\n",
    "df1 = pd.DataFrame({\"A\":[\"A1\", \"A2\"],\n",
    "                    \"B\":[\"B1\",\"B2\"]},index=[1,2])\n",
    "df2 = pd.DataFrame({\"C\":[\"C1\", \"C2\"],\n",
    "                    \"D\":[\"D1\",\"D2\"]},index=[1,2])\n",
    "pd.concat([df1,df2], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* And pandas will gracefully handle mis-alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What happens when indexes don't line up\n",
    "df1 = pd.DataFrame({\"A\":[\"A1\", \"A2\"],\n",
    "                    \"B\":[\"B1\",\"B2\"]},index=[1,2])\n",
    "df2 = pd.DataFrame({\"A\":[\"A3\", \"A4\"],\n",
    "                    \"B\":[\"B3\",\"B4\"]},index=[3,4])\n",
    "pd.concat([df1,df2], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The `append` function is a method of a Series/Dataframe and returns a new object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append df2 to df1\n",
    "df1.append(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging and Joining\n",
    "\n",
    "* While `concat()` is useful it lacks the power to do complex data merging\n",
    "* For example, I have two tables of different data but one shared column\n",
    "* This is where the `merge()` function becomes useful because it lets you *join* datasets\n",
    "* The concept of \"join\" has lots of theory and is a richly developed method for *joining* data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-to-one joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create two dataframes with one shared column\n",
    "df1 = pd.DataFrame({'employee': ['Bob', 'Jake', 'Lisa', 'Sue', \"Nancy\"],\n",
    "                    'group': ['Accounting', 'Engineering', 'Engineering', 'HR', \"Librarian\"]})\n",
    "df2 = pd.DataFrame({'employee': ['Lisa', 'Bob', 'Jake', 'Sue'],\n",
    "                    'hire_date': [2004, 2008, 2012, 2014]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display df1\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display df2\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge df1 and df2 into a new dataframe df3\n",
    "df3 = pd.merge(df1, df2)\n",
    "df3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The new dataframe `df3` now has all of the data from df1 and df2\n",
    "* The `merge` function automatically connected the two tables on the \"employee\" column\n",
    "* But what happens when your data don't line up?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Many-to-one joins\n",
    "\n",
    "* Sometimes there isn't a one to one relationshp between rows in  two datasets\n",
    "* A *many-to-one* join lets you combine these datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make another dataframe about the supervisor for each group\n",
    "df4 = pd.DataFrame({'group': ['Accounting', 'Engineering', 'HR'],\n",
    "                    'supervisor': ['Carly', 'Guido', 'Steve']})\n",
    "df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge df3 from above with the supervisor info in df4\n",
    "pd.merge(df3,df4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Notice how the information about Guido, the manager for Engineering, is repeated.\n",
    "* Pandas automatically fills in these values to maintain the tabular, 2 dimensional structure of the data\n",
    "* While this might seem like duplicated data, it makes it easier to quickly look up Jake and Lisa's supervisor without consulting multiple tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Many-to-many joins\n",
    "\n",
    "* Let's combine the employee information with skills information\n",
    "* Notice there isn't a one to one or even a one to many relationship between these tables\n",
    "* Each group can have multiple skills, so **what do you think will happen?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the employee table specified above\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new dataframe with skills information\n",
    "df5 = pd.DataFrame({'group': ['Accounting', 'Accounting',\n",
    "                              'Engineering', 'Engineering', 'HR', 'HR', 'Librarian'],\n",
    "                    'skills': ['math', 'spreadsheets', 'coding', 'linux',\n",
    "                               'spreadsheets', 'organization', 'nunchucks']})\n",
    "df5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(df1, df5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Amazing, Pandas merge capabilities are very useful when column names match up\n",
    "* But what do you do if the names of your columns don't match?\n",
    "* You could change column names...\n",
    "* But that is crazy! Just use the `left_on` and `right_on` parameters to the `merge()` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the employee table specified above\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename the column \"employee\" to \"name\"\n",
    "df2 = df2.rename({\"employee\":\"name\"}, axis=\"columns\")\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets try and merge them without specifying what to merge on\n",
    "pd.merge(df1, df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Gak, error! Pandas can't figure out how to combine them\n",
    "* What are the column names I should specify?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets specify the column name \n",
    "pd.merge(df1, df2, left_on=\"employee\", right_on=\"name\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Notice we now have a redundant employee/name column, this is a by-product of merging different columns\n",
    "* If you want to get rid of it you can use the `drop` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the name column, axis=1 means axis='col', which is confusing\n",
    "pd.merge(df1, df2, left_on=\"employee\", right_on=\"name\" ).drop(\"name\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joining with Real Data\n",
    "\n",
    "The example above was illustrative, but it might be useful to see how you might join two *real* datasets together \n",
    "\n",
    "We have two data files, `service_requests.csv`, which contains 311 requests and `311-codebook.csv`, which maps request types/issues to higher level categories. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the 311 data\n",
    "service_requests = pd.read_csv(\"files/service_requests.csv\")\n",
    "service_requests.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at all the unique values for REQUEST TYPE\n",
    "service_requests[\"REQUEST_TYPE\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many unique request types are there?\n",
    "len(service_requests[\"REQUEST_TYPE\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we are aggregating our data, 294 separate issues is still a lot of data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the 311 codebook data\n",
    "code_book = pd.read_csv(\"files/311-codebook.csv\")\n",
    "code_book.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we can do some fancy merging magic. We know there is some overlap in the data column between the 311 requests and the codebook. Basically, the data in the `REQUEST_TYPE` column of the 311 data shares the same values as the `Issue` column of the code book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new, merged dataframe t\n",
    "merged_df = pd.merge(service_requests, code_book, left_on=\"REQUEST_TYPE\", right_on=\"Issue\")\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are all the unique values for Category\n",
    "merged_df[\"Category\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many categories\n",
    "len(merged_df[\"Category\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of service requests for each category\n",
    "merged_df[\"Category\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a much more comprehendable aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouping Data\n",
    "\n",
    "\n",
    "* A common pattern in data analysis is splitting data by a key and then performing some math on all of the values with that key and finally combining it all back together\n",
    "* This is commonly known in data circles as *split, apply, combine*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe to illustrate GroupBy\n",
    "df = pd.DataFrame({'key': ['A', 'B', 'C', 'A', 'B', 'C'],\n",
    "                   'data': range(6),\n",
    "                   'counts':[45,234,6,2,1324,345], \n",
    "                   'things':['dog', 'cat', 'cat', 'dog', 'cat', 'cat']}\n",
    "                 )\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Under the hood Pandas is creating a bunch of new Dataframes based on the grouping column values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over each group of data, don't do this at home\n",
    "for group in df.groupby('key'):\n",
    "    print(\"Group for key:\", group[0])\n",
    "    print(\"Data:\", group[1])\n",
    "    print(\"Data Type:\", type(group[1]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Cool, we can see that we have *split* our data into three groups\n",
    "* Now, we need to tell Pandas what function to *apply* to each group\n",
    "* We need to specify what kind of aggregation, transformation, or computation to perform on the group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tell pandas to add up all of the values for each key\n",
    "df.groupby('key').sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Pandas will apply the aggregation function only to relevant columns\n",
    "* Mathy functions will only be applied to numerical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can save the group object and run different aggregations\n",
    "grouped_dataframe = df.groupby('key')\n",
    "grouped_dataframe.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_dataframe.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_dataframe.prod()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The following table summarizes some other built-in Pandas aggregations:\n",
    "\n",
    "| Aggregation              | Description                     |\n",
    "|--------------------------|---------------------------------|\n",
    "| ``count()``              | Total number of items           |\n",
    "| ``size()``               | Total number of items w/ NaNs   |\n",
    "| ``first()``, ``last()``  | First and last item             |\n",
    "| ``mean()``, ``median()`` | Mean and median                 |\n",
    "| ``min()``, ``max()``     | Minimum and maximum             |\n",
    "| ``std()``, ``var()``     | Standard deviation and variance |\n",
    "| ``mad()``                | Mean absolute deviation         |\n",
    "| ``prod()``               | Product of all items            |\n",
    "| ``sum()``                | Sum of all items                |\n",
    "\n",
    "* These are all methods of ``DataFrame`` and ``Series`` objects.\n",
    "\n",
    "* You can also do multiple levels of grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['things','key']).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* What you are seeing is what is called a [Multilevel Index](https://pandas.pydata.org/pandas-docs/stable/advanced.html)\n",
    "* Sadly, we don't have time to cover that topic, but this chapter on [Hierarchical Indexing](https://jakevdp.github.io/PythonDataScienceHandbook/03.05-hierarchical-indexing.html) in the [Python Data Science Handbook](https://jakevdp.github.io/PythonDataScienceHandbook/) is a great introduction to the topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split, Apply, Combine with Real Data\n",
    "\n",
    "Let's try and analyze some real data about the all those staircases in Pittsburgh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the PGH steps data\n",
    "steps = pd.read_csv(\"files/steps.csv\")\n",
    "steps.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the data loaded into pandas we can start asking questions of our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many steps are there total in PGH?\n",
    "steps[\"number_of_steps\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of staircases with each material type\n",
    "steps['material'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we want to break this down by neighborhood?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by neighborhood, then count the material types and display 50 values\n",
    "steps.groupby(\"neighborhood\")['material'].value_counts().head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could also use indexing to grab values for a particular neighborhood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by neighborhood, then count the material types and display 50 values\n",
    "steps.groupby(\"neighborhood\")['material'].value_counts().loc['Greenfield']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the average number of steps\n",
    "steps[\"number_of_steps\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the average number of steps by material\n",
    "steps.groupby(\"material\")['number_of_steps'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What neighborhood has the most number of steps\n",
    "steps.groupby(\"neighborhood\")[\"number_of_steps\"].sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the longest staircase by type per neighborhood\n",
    "steps.groupby([\"neighborhood\", \"material\"])[\"number_of_steps\"].max().head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
